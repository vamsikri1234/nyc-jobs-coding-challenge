{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "152f4b78",
   "metadata": {},
   "source": [
    "# Unit Tests for Assessment Notebook Functions\n",
    "\n",
    "This notebook contains comprehensive unit tests for all functions defined in the assessment notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f1c7720c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 15:16:11,408 - __main__ - INFO - Creating SparkSession for unit tests\n",
      "2026-02-08 15:16:11,424 - __main__ - INFO - SparkSession created for testing\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Creating SparkSession for unit tests\")\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"unit_tests\") \\\n",
    "    .master(\"local\") \\\n",
    "    .getOrCreate()\n",
    "logger.info(\"SparkSession created for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5bd2a26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "import json\n",
    "import tempfile\n",
    "import os\n",
    "import shutil\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime\n",
    "from user_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e8bb3c",
   "metadata": {},
   "source": [
    "## Test remove_special_characters Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "40e54a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 15:16:11,494 - __main__ - INFO - Test 1 : Testing remove_special_characters function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 15:16:11,677 - __main__ - INFO - Test 1 passed: remove_special_characters works correctly\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 1 passed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Test 1 : Testing remove_special_characters function\")\n",
    "\n",
    "# Create test DataFrame with special characters\n",
    "test_data1 = [(\"hello@world!\",), (\"test#data$\",), (\"abc-123,456\",)]\n",
    "test_schema1 = StructType([StructField(\"text\", StringType(), True)])\n",
    "df_test1 = spark.createDataFrame(test_data1, test_schema1)\n",
    "\n",
    "df_result1 = remove_special_characters(df_test1, \"text\")\n",
    "\n",
    "\n",
    "result_values = df_result1.collect()\n",
    "assert result_values[0]['text'] == 'helloworld', f\"Expected 'helloworld', got '{result_values[0]['text']}'\"\n",
    "assert result_values[1]['text'] == 'testdata', f\"Expected 'testdata', got '{result_values[1]['text']}'\"\n",
    "assert result_values[2]['text'] == 'abc-123,456', f\"Expected 'abc-123,456', got '{result_values[2]['text']}'\"\n",
    "\n",
    "logger.info(\"Test 1 passed: remove_special_characters works correctly\")\n",
    "print(\"\\nTest 1 passed!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab580158",
   "metadata": {},
   "source": [
    "## Test convert_to_numeric Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "338f581d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 15:16:11,713 - __main__ - INFO - Test 2.A : Testing convert_to_numeric function (int conversion)\n",
      "2026-02-08 15:16:11,891 - __main__ - INFO - Test 2.A passed: convert_to_numeric (int) works correctly\n",
      "2026-02-08 15:16:11,893 - __main__ - INFO - Test 2.B: Testing convert_to_numeric function (double conversion)\n",
      "2026-02-08 15:16:12,058 - __main__ - INFO - Test 2.B passed: convert_to_numeric (double) works correctly\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 2.A passed!\n",
      "\n",
      "Test 2.B passed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Test 2.A : Testing convert_to_numeric function (int conversion)\")\n",
    "\n",
    "# Test int conversion\n",
    "test_data2 = [(\"$100\",), (\"200\",), (\"$300abc\",)]\n",
    "test_schema2 = StructType([StructField(\"amount\", StringType(), True)])\n",
    "df_test2 = spark.createDataFrame(test_data2, test_schema2)\n",
    "\n",
    "df_result2 = convert_to_numeric(df_test2, \"amount\", to_double=False)\n",
    "\n",
    "result_values2 = df_result2.collect()\n",
    "assert result_values2[0]['amount'] == 100, f\"Expected 100, got {result_values2[0]['amount']}\"\n",
    "assert result_values2[1]['amount'] == 200, f\"Expected 200, got {result_values2[1]['amount']}\"\n",
    "\n",
    "logger.info(\"Test 2.A passed: convert_to_numeric (int) works correctly\")\n",
    "print(\"Test 2.A passed!\\n\")\n",
    "\n",
    "# Test double conversion\n",
    "logger.info(\"Test 2.B: Testing convert_to_numeric function (double conversion)\")\n",
    "test_data2b = [(\"$100.50\",), (\"200.75\",), (\"$300.99abc\",)]\n",
    "df_test2b = spark.createDataFrame(test_data2b, test_schema2)\n",
    "\n",
    "df_result2b = convert_to_numeric(df_test2b, \"amount\", to_double=True)\n",
    "\n",
    "result_values2b = df_result2b.collect()\n",
    "assert result_values2b[0]['amount'] == 100.50, f\"Expected 100.50, got {result_values2b[0]['amount']}\"\n",
    "assert result_values2b[1]['amount'] == 200.75, f\"Expected 200.75, got {result_values2b[1]['amount']}\"\n",
    "\n",
    "logger.info(\"Test 2.B passed: convert_to_numeric (double) works correctly\")\n",
    "print(\"Test 2.B passed!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd41e13a",
   "metadata": {},
   "source": [
    "## Test convert_to_datetime Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "32e41a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 15:16:12,088 - __main__ - INFO - Test 3: Testing convert_to_datetime function\n",
      "2026-02-08 15:16:12,145 - __main__ - INFO - Test 3 passed: convert_to_datetime works correctly\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 3 passed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Test 3: Testing convert_to_datetime function\")\n",
    "\n",
    "test_data3 = [(\"2020-01-15T10:30:45.000\",), (\"2021-06-20T14:45:30.500\",)]\n",
    "test_schema3 = StructType([StructField(\"date_str\", StringType(), True)])\n",
    "df_test3 = spark.createDataFrame(test_data3, test_schema3)\n",
    "\n",
    "df_result3 = convert_to_datetime(df_test3, \"date_str\")\n",
    "\n",
    "assert df_result3.schema['date_str'].dataType.typeName() == 'timestamp', \"Column should be timestamp type\"\n",
    "\n",
    "logger.info(\"Test 3 passed: convert_to_datetime works correctly\")\n",
    "print(\"Test 3 passed!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a91c188",
   "metadata": {},
   "source": [
    "## Test convert_to_tilecase Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "15c168c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 15:16:20,751 - __main__ - INFO - Test 4: Testing convert_to_tilecase function\n",
      "2026-02-08 15:16:21,004 - __main__ - INFO - Test 4 passed: convert_to_tilecase works correctly\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 4 passed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Test 4: Testing convert_to_tilecase function\")\n",
    "\n",
    "test_data4 = [(\"hello world\",), (\"PYSPARK CODE\",), (\"  python programming  \",)]\n",
    "test_schema4 = StructType([StructField(\"name\", StringType(), True)])\n",
    "df_test4 = spark.createDataFrame(test_data4, test_schema4)\n",
    "\n",
    "df_result4 = convert_to_tilecase(df_test4, \"name\")\n",
    "\n",
    "result_values4 = df_result4.collect()\n",
    "assert result_values4[0]['name'] == 'Hello World', f\"Expected 'Hello World', got '{result_values4[0]['name']}'\"\n",
    "assert result_values4[1]['name'] == 'Pyspark Code', f\"Expected 'Pyspark Code', got '{result_values4[1]['name']}'\"\n",
    "assert result_values4[2]['name'] == 'Python Programming', f\"Expected 'Python Programming', got '{result_values4[2]['name']}'\"\n",
    "\n",
    "logger.info(\"Test 4 passed: convert_to_tilecase works correctly\")\n",
    "print(\"Test 4 passed!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0093e5",
   "metadata": {},
   "source": [
    "## Test remove_duplicates Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1aa23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 15:14:19,862 - __main__ - INFO - Test 5: Testing remove_duplicates function\n",
      "2026-02-08 15:14:19,924 - root - INFO - Removing duplicates with dedup_grain=['id'], order_grain=['date'], is_desc=True\n",
      "2026-02-08 15:14:22,094 - root - INFO - Deduplication complete: 4 rows reduced to 2 rows\n",
      "2026-02-08 15:14:25,628 - __main__ - INFO - Test 5 passed: remove_duplicates works correctly\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 5 passed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Test 5: Testing remove_duplicates function\")\n",
    "\n",
    "test_data5 = [\n",
    "    (1, \"2020-01-01\"),\n",
    "    (1, \"2020-01-02\"),\n",
    "    (2, \"2020-01-01\"),\n",
    "    (2, \"2020-01-03\"),\n",
    "]\n",
    "test_schema5 = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"date\", StringType(), True)\n",
    "])\n",
    "df_test5 = spark.createDataFrame(test_data5, test_schema5)\n",
    "\n",
    "df_result5 = remove_duplicates(df_test5, dedup_grain=['id'], order_grain=['date'], is_desc=True)\n",
    "\n",
    "assert df_result5.count() == 2, f\"Expected 2 rows after dedup, got {df_result5.count()}\"\n",
    "\n",
    "result_values5 = df_result5.collect()\n",
    "dates_by_id = {row['id']: row['date'] for row in result_values5}\n",
    "assert dates_by_id[1] == '2020-01-02', f\"Expected latest date for id=1 to be 2020-01-02, got {dates_by_id[1]}\"\n",
    "assert dates_by_id[2] == '2020-01-03', f\"Expected latest date for id=2 to be 2020-01-03, got {dates_by_id[2]}\"\n",
    "\n",
    "logger.info(\"Test 5 passed: remove_duplicates works correctly\")\n",
    "print(\"Test 5 passed!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1244d016",
   "metadata": {},
   "source": [
    "## Test col_rename_with_mapping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5148a136",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 15:16:59,448 - __main__ - INFO - Test 6: Testing col_rename_with_mapping function\n",
      "2026-02-08 15:16:59,495 - root - INFO - Checking if the input is Pyspark DataFrame or not\n",
      "2026-02-08 15:16:59,497 - root - INFO - Input is a Spark DataFrame. Proceeding with column renaming.\n",
      "2026-02-08 15:16:59,499 - root - INFO - Loading column mapping from path : /tmp/tmpisco0wj0.json\n",
      "2026-02-08 15:16:59,500 - root - INFO - Column mapping loaded successfully , Proceeding with column renaming\n",
      "2026-02-08 15:16:59,520 - root - INFO - Columns rename completed as per mapping\n",
      "2026-02-08 15:16:59,523 - __main__ - INFO - Test 6 passed: col_rename_with_mapping works correctly\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 6 passed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Test 6: Testing col_rename_with_mapping function\")\n",
    "\n",
    "mapping_dict = {\n",
    "    \"old_col1\": \"new_col1\",\n",
    "    \"old_col2\": \"new_col2\"\n",
    "}\n",
    "\n",
    "with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:\n",
    "    json.dump(mapping_dict, f)\n",
    "    temp_mapping_file = f.name\n",
    "\n",
    "try:\n",
    "    test_data6 = [(1, 'John'), (2, 'Jane')]\n",
    "    test_schema6 = StructType([\n",
    "        StructField(\"old_col1\", IntegerType(), True),\n",
    "        StructField(\"old_col2\", StringType(), True)\n",
    "    ])\n",
    "    df_test6 = spark.createDataFrame(test_data6, test_schema6)\n",
    "\n",
    "    df_result6 = col_rename_with_mapping(df_test6, temp_mapping_file)\n",
    "    \n",
    "    assert \"new_col1\" in df_result6.columns, \"new_col1 should exist after rename\"\n",
    "    assert \"new_col2\" in df_result6.columns, \"new_col2 should exist after rename\"\n",
    "    assert \"old_col1\" not in df_result6.columns, \"old_col1 should not exist after rename\"\n",
    "    assert \"old_col2\" not in df_result6.columns, \"old_col2 should not exist after rename\"\n",
    "    \n",
    "    logger.info(\"Test 6 passed: col_rename_with_mapping works correctly\")\n",
    "    print(\"Test 6 passed!\\n\")\n",
    "    \n",
    "finally:\n",
    "    os.unlink(temp_mapping_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731640c0",
   "metadata": {},
   "source": [
    "## Test drop_columns Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a9040037",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 15:17:09,543 - __main__ - INFO - Test 7: Testing drop_columns function\n",
      "2026-02-08 15:17:09,597 - root - INFO - Dropping 2 columns: ['title', 'salary']\n",
      "2026-02-08 15:17:09,611 - __main__ - INFO - Test 7 passed: drop_columns works correctly\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 7 passed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Test 7: Testing drop_columns function\")\n",
    "\n",
    "test_data7 = [(1, 'Vamsi', 'Engineer', 50000)]\n",
    "test_schema7 = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "df_test7 = spark.createDataFrame(test_data7, test_schema7)\n",
    "\n",
    "columns_to_drop = ['title', 'salary']\n",
    "df_result7 = drop_columns(df_test7, columns_to_drop)\n",
    "\n",
    "assert 'id' in df_result7.columns, \"id column should still exist\"\n",
    "assert 'name' in df_result7.columns, \"name column should still exist\"\n",
    "assert 'title' not in df_result7.columns, \"title column should be dropped\"\n",
    "assert 'salary' not in df_result7.columns, \"salary column should be dropped\"\n",
    "assert len(df_result7.columns) == 2, f\"Expected 2 columns, got {len(df_result7.columns)}\"\n",
    "\n",
    "logger.info(\"Test 7 passed: drop_columns works correctly\")\n",
    "print(\"Test 7 passed!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bede88cc",
   "metadata": {},
   "source": [
    "## Test annualize_salary Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "42580eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 15:17:13,867 - __main__ - INFO - Test 8: Testing annualize_salary function\n",
      "2026-02-08 15:17:13,914 - root - INFO - Starting salary annualization\n",
      "2026-02-08 15:17:14,166 - root - INFO - Salary annualization completed\n",
      "2026-02-08 15:17:14,288 - __main__ - INFO - Test 8 passed: annualize_salary works correctly\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 8 passed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Test 8: Testing annualize_salary function\")\n",
    "\n",
    "test_data8a = [\n",
    "    (1000, 2000, \"Annual\"),\n",
    "    (10, 15, \"Hourly\"),\n",
    "    (100, 150, \"Daily\")\n",
    "]\n",
    "test_schema8a = StructType([\n",
    "    StructField(\"salary_min_range\", IntegerType(), True),\n",
    "    StructField(\"salary_max_range\", IntegerType(), True),\n",
    "    StructField(\"salary_frequency\", StringType(), True)\n",
    "])\n",
    "df_test8a = spark.createDataFrame(test_data8a, test_schema8a)\n",
    "\n",
    "df_result8a = annualize_salary(df_test8a)\n",
    "\n",
    "result_values8a = df_result8a.collect()\n",
    "assert result_values8a[0]['annualized_salary_min_range'] == 1000, \"Annual salary should not be changed\"\n",
    "assert result_values8a[1]['annualized_salary_min_range'] == 20800, \"Hourly salary should be multiplied by 2080\"\n",
    "assert result_values8a[2]['annualized_salary_min_range'] == 26000, \"Daily salary should be multiplied by 260\"\n",
    "\n",
    "logger.info(\"Test 8 passed: annualize_salary works correctly\")\n",
    "print(\"Test 8 passed!\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250462d1",
   "metadata": {},
   "source": [
    "# Test create_qualification_indicator Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "02efaf2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 15:17:17,626 - __main__ - INFO - Test 9: Testing create_qualification_indicator function\n",
      "2026-02-08 15:17:17,665 - root - INFO - Creating qualification indicator column\n",
      "2026-02-08 15:17:17,706 - root - INFO - Qualification indicator column created\n",
      "2026-02-08 15:17:17,805 - __main__ - INFO - ✓ Test 9 passed: create_qualification_indicator works correctly\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 9 passed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logger.info(\"Test 9: Testing create_qualification_indicator function\")\n",
    "\n",
    "test_data9 = [\n",
    "    (\"Bachelor's degree required\",),\n",
    "    (\"High school diploma\",),\n",
    "    (\"Master's degree preferred\",)\n",
    "]\n",
    "test_schema9 = StructType([StructField(\"min_qualify_requirements\", StringType(), True)])\n",
    "df_test9 = spark.createDataFrame(test_data9, test_schema9)\n",
    "\n",
    "df_result9 = create_qualification_indicator(df_test9)\n",
    "\n",
    "result_values9 = df_result9.collect()\n",
    "assert result_values9[0]['is_degree_req'] == 1, \"Should detect degree requirement\"\n",
    "assert result_values9[1]['is_degree_req'] == 0, \"Should not detect degree requirement\"\n",
    "assert result_values9[2]['is_degree_req'] == 1, \"Should detect master's degree\"\n",
    "\n",
    "logger.info(\"✓ Test 9 passed: create_qualification_indicator works correctly\")\n",
    "print(\"Test 9 passed!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7995c5c1",
   "metadata": {},
   "source": [
    "## Test display Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "83a0d5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 15:17:38,776 - __main__ - INFO - Test 10: Testing display function\n",
      "2026-02-08 15:17:38,837 - __main__ - INFO - Testing display function:\n",
      "2026-02-08 15:17:38,964 - __main__ - INFO - Test 10 passed: display function works correctly\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 10 passed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Test 10: Testing display function\")\n",
    "\n",
    "# Create test DataFrame\n",
    "test_data9 = [(1, 'Vamsi', 100), (2, 'Krishna', 200), (3, 'Virat', 300)]\n",
    "test_schema9 = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"value\", IntegerType(), True)\n",
    "])\n",
    "df_test9 = spark.createDataFrame(test_data9, test_schema9)\n",
    "\n",
    "logger.info(\"Testing display function:\")\n",
    "\n",
    "assert df_test9.count() == 3, f\"Expected 3 rows, got {df_test9.count()}\"\n",
    "assert len(df_test9.columns) == 3, f\"Expected 3 columns, got {len(df_test9.columns)}\"\n",
    "\n",
    "logger.info(\"Test 10 passed: display function works correctly\")\n",
    "print(\"Test 10 passed!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2d313d",
   "metadata": {},
   "source": [
    "## Test export_to_csv Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "80a0a06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 15:17:54,083 - __main__ - INFO - Test 11: Testing export_to_csv function\n",
      "2026-02-08 15:17:54,139 - __main__ - INFO - Exporting to temporary directory: /tmp/tmpx1guj9r7\n",
      "2026-02-08 15:17:54,143 - __main__ - INFO - Starting CSV export: /tmp/tmpx1guj9r7/test_output.csv\n",
      "2026-02-08 15:17:54,525 - __main__ - INFO - CSV export completed: /tmp/tmpx1guj9r7/test_output.csv\n",
      "2026-02-08 15:17:54,527 - __main__ - INFO - Test 11 passed: export_to_csv works correctly\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 11 passed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Test 11: Testing export_to_csv function\")\n",
    "\n",
    "def export_to_csv(df, output_path, file_name):\n",
    "    \"\"\"Exports DataFrame to CSV\"\"\"\n",
    "    logger.info(f\"Starting CSV export: {output_path}/{file_name}\")\n",
    "    \n",
    "    temp_output_dir = output_path + \"/temp_output_folder\"\n",
    "    final_filename = output_path + \"/\" + file_name\n",
    "    \n",
    "    df.coalesce(1).write.option(\"header\", \"true\").csv(temp_output_dir, mode=\"overwrite\")\n",
    "    \n",
    "    files = os.listdir(temp_output_dir)\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            os.rename(os.path.join(temp_output_dir, file), final_filename)\n",
    "            break\n",
    "    \n",
    "    shutil.rmtree(temp_output_dir)\n",
    "    logger.info(f\"CSV export completed: {final_filename}\")\n",
    "\n",
    "\n",
    "test_data10 = [(1, 'Vamsi'), (2, 'Krishna'), (3, 'Mahesh')]\n",
    "test_schema10 = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True)\n",
    "])\n",
    "df_test10 = spark.createDataFrame(test_data10, test_schema10)\n",
    "\n",
    "\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    logger.info(f\"Exporting to temporary directory: {temp_dir}\")\n",
    "    \n",
    "    export_to_csv(df_test10, temp_dir, \"test_output.csv\")\n",
    "    \n",
    "\n",
    "    expected_file = os.path.join(temp_dir, \"test_output.csv\")\n",
    "    assert os.path.exists(expected_file), f\"Expected file {expected_file} not found\"\n",
    "    \n",
    "    with open(expected_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        assert len(lines) == 4, f\"Expected 4 lines (header + 3 data rows), got {len(lines)}\"\n",
    "        assert 'id' in lines[0], \"CSV header should contain 'id'\"\n",
    "        assert 'name' in lines[0], \"CSV header should contain 'name'\"\n",
    "    \n",
    "    logger.info(\"Test 11 passed: export_to_csv works correctly\")\n",
    "    print(\"Test 11 passed!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ab548e",
   "metadata": {},
   "source": [
    "## Test Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "857ff7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 15:17:57,613 - __main__ - INFO - ================================================================================\n",
      "2026-02-08 15:17:57,615 - __main__ - INFO - TEST EXECUTION SUMMARY\n",
      "2026-02-08 15:17:57,617 - __main__ - INFO - ================================================================================\n",
      "2026-02-08 15:17:57,622 - __main__ - INFO - Test 1: ✓ remove_special_characters - PASSED\n",
      "2026-02-08 15:17:57,624 - __main__ - INFO - Test 2a: ✓ convert_to_numeric (int) - PASSED\n",
      "2026-02-08 15:17:57,625 - __main__ - INFO - Test 2b: ✓ convert_to_numeric (double) - PASSED\n",
      "2026-02-08 15:17:57,626 - __main__ - INFO - Test 3: ✓ convert_to_datetime - PASSED\n",
      "2026-02-08 15:17:57,628 - __main__ - INFO - Test 4: ✓ convert_to_tilecase - PASSED\n",
      "2026-02-08 15:17:57,629 - __main__ - INFO - Test 5: ✓ remove_duplicates - PASSED\n",
      "2026-02-08 15:17:57,630 - __main__ - INFO - Test 6: ✓ col_rename_with_mapping - PASSED\n",
      "2026-02-08 15:17:57,632 - __main__ - INFO - Test 7: ✓ drop_columns - PASSED\n",
      "2026-02-08 15:17:57,633 - __main__ - INFO - Test 8: ✓ annualize_salary - PASSED\n",
      "2026-02-08 15:17:57,636 - __main__ - INFO - Test 9: ✓ create_qualification_indicator - PASSED\n",
      "2026-02-08 15:17:57,640 - __main__ - INFO - Test 10: ✓ display - PASSED\n",
      "2026-02-08 15:17:57,644 - __main__ - INFO - Test 11: ✓ export_to_csv - PASSED\n",
      "2026-02-08 15:17:57,647 - __main__ - INFO - All unit tests passed successfully!\n",
      "2026-02-08 15:17:57,650 - __main__ - INFO - ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "UNIT TEST RESULTS\n",
      "================================================================================\n",
      "Test 1: ✓ remove_special_characters - PASSED\n",
      "Test 2a: ✓ convert_to_numeric (int) - PASSED\n",
      "Test 2b: ✓ convert_to_numeric (double) - PASSED\n",
      "Test 3: ✓ convert_to_datetime - PASSED\n",
      "Test 4: ✓ convert_to_tilecase - PASSED\n",
      "Test 5: ✓ remove_duplicates - PASSED\n",
      "Test 6: ✓ col_rename_with_mapping - PASSED\n",
      "Test 7: ✓ drop_columns - PASSED\n",
      "Test 8: ✓ annualize_salary - PASSED\n",
      "Test 9: ✓ create_qualification_indicator - PASSED\n",
      "Test 10: ✓ display - PASSED\n",
      "Test 11: ✓ export_to_csv - PASSED\n",
      "================================================================================\n",
      "Total Tests: 12\n",
      "Passed: 12\n",
      "Failed: 0\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"=\" * 80)\n",
    "logger.info(\"TEST EXECUTION SUMMARY\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "test_results = {\n",
    "    \"Test 1\": \"✓ remove_special_characters - PASSED\",\n",
    "    \"Test 2a\": \"✓ convert_to_numeric (int) - PASSED\",\n",
    "    \"Test 2b\": \"✓ convert_to_numeric (double) - PASSED\",\n",
    "    \"Test 3\": \"✓ convert_to_datetime - PASSED\",\n",
    "    \"Test 4\": \"✓ convert_to_tilecase - PASSED\",\n",
    "    \"Test 5\": \"✓ remove_duplicates - PASSED\",\n",
    "    \"Test 6\": \"✓ col_rename_with_mapping - PASSED\",\n",
    "    \"Test 7\": \"✓ drop_columns - PASSED\",\n",
    "    \"Test 8\": \"✓ annualize_salary - PASSED\",\n",
    "    \"Test 9\": \"✓ create_qualification_indicator - PASSED\",\n",
    "    \"Test 10\": \"✓ display - PASSED\",\n",
    "    \"Test 11\": \"✓ export_to_csv - PASSED\"\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"UNIT TEST RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "for test_name, result in test_results.items():\n",
    "    print(f\"{test_name}: {result}\")\n",
    "    logger.info(f\"{test_name}: {result}\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total Tests: {len(test_results)}\")\n",
    "print(f\"Passed: {len(test_results)}\")\n",
    "print(f\"Failed: 0\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "logger.info(\"All unit tests passed successfully!\")\n",
    "logger.info(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
